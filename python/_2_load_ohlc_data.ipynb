{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b1ca7370",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.3.3\n",
      "0.2.66\n",
      "2.32.5\n",
      "1.0\n",
      "2.9.11 (dt dec pq3 ext lo64)\n",
      "2.0.44\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import yfinance as yf\n",
    "import requests\n",
    "import csv\n",
    "import psycopg2\n",
    "import sqlalchemy\n",
    "\n",
    "print(pd.__version__)\n",
    "print(yf.__version__)\n",
    "print(requests.__version__)\n",
    "print(csv.__version__)\n",
    "print(psycopg2.__version__)\n",
    "print(sqlalchemy.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6400e89e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read stock symbols from the file\n",
    "with open('sp500_symbols.txt', 'r') as file:\n",
    "    stock_symbols = [line.strip() for line in file.readlines()]\n",
    "print(f\"Loaded {len(stock_symbols)} stock symbols.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "e83a0438",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Existing CSV: 501 symbols\n",
      "Renewing 144 symbols...\n",
      "[1/144] PLTR OK\n",
      "[2/144] PANW OK\n",
      "[3/144] PSKY OK\n",
      "[4/144] PH OK\n",
      "[5/144] PAYX OK\n",
      "[6/144] PAYC OK\n",
      "[7/144] PYPL OK\n",
      "[8/144] PNR OK\n",
      "[9/144] PEP OK\n",
      "[10/144] PFE OK\n",
      "[11/144] PCG OK\n",
      "[12/144] PM OK\n",
      "[13/144] PSX OK\n",
      "[14/144] PNW OK\n",
      "[15/144] PNC OK\n",
      "[16/144] POOL OK\n",
      "[17/144] PPG OK\n",
      "[18/144] PPL OK\n",
      "[19/144] PFG OK\n",
      "[20/144] PG OK\n",
      "[21/144] PGR OK\n",
      "[22/144] PLD OK\n",
      "[23/144] PRU OK\n",
      "[24/144] PEG OK\n",
      "[25/144] PTC OK\n",
      "[26/144] PSA OK\n",
      "[27/144] PHM OK\n",
      "[28/144] PWR OK\n",
      "[29/144] QCOM OK\n",
      "[30/144] DGX OK\n",
      "[31/144] RL OK\n",
      "[32/144] RJF OK\n",
      "[33/144] RTX OK\n",
      "[34/144] O OK\n",
      "[35/144] REG OK\n",
      "[36/144] REGN OK\n",
      "[37/144] RF OK\n",
      "[38/144] RSG OK\n",
      "[39/144] RMD OK\n",
      "[40/144] RVTY OK\n",
      "[41/144] ROK OK\n",
      "[42/144] ROL OK\n",
      "[43/144] ROP OK\n",
      "[44/144] ROST OK\n",
      "[45/144] RCL OK\n",
      "[46/144] SPGI OK\n",
      "[47/144] CRM OK\n",
      "[48/144] SBAC OK\n",
      "[49/144] SLB OK\n",
      "[50/144] STX OK\n",
      "[51/144] SRE OK\n",
      "[52/144] NOW OK\n",
      "[53/144] SHW OK\n",
      "[54/144] SPG OK\n",
      "[55/144] SWKS OK\n",
      "[56/144] SJM OK\n",
      "[57/144] SW OK\n",
      "[58/144] SNA OK\n",
      "[59/144] SOLV OK\n",
      "[60/144] SO OK\n",
      "[61/144] LUV OK\n",
      "[62/144] SWK OK\n",
      "[63/144] SBUX OK\n",
      "[64/144] STT OK\n",
      "[65/144] STLD OK\n",
      "[66/144] STE OK\n",
      "[67/144] SYK OK\n",
      "[68/144] SMCI OK\n",
      "[69/144] SYF OK\n",
      "[70/144] SNPS OK\n",
      "[71/144] SYY OK\n",
      "[72/144] TMUS OK\n",
      "[73/144] TROW OK\n",
      "[74/144] TTWO OK\n",
      "[75/144] TPR OK\n",
      "[76/144] TRGP OK\n",
      "[77/144] TGT OK\n",
      "[78/144] TEL OK\n",
      "[79/144] TDY OK\n",
      "[80/144] TER OK\n",
      "[81/144] TSLA OK\n",
      "[82/144] TXN OK\n",
      "[83/144] TPL OK\n",
      "[84/144] TXT OK\n",
      "[85/144] TMO OK\n",
      "[86/144] TJX OK\n",
      "[87/144] TKO OK\n",
      "[88/144] TTD OK\n",
      "[89/144] TSCO OK\n",
      "[90/144] TT OK\n",
      "[91/144] TDG OK\n",
      "[92/144] TRV OK\n",
      "[93/144] TRMB OK\n",
      "[94/144] TFC OK\n",
      "[95/144] TYL OK\n",
      "[96/144] TSN OK\n",
      "[97/144] USB OK\n",
      "[98/144] UBER OK\n",
      "[99/144] UDR OK\n",
      "[100/144] ULTA OK\n",
      "[101/144] UNP OK\n",
      "[102/144] UAL OK\n",
      "[103/144] UPS OK\n",
      "[104/144] URI OK\n",
      "[105/144] UNH OK\n",
      "[106/144] UHS OK\n",
      "[107/144] VLO OK\n",
      "[108/144] VTR OK\n",
      "[109/144] VLTO OK\n",
      "[110/144] VRSN OK\n",
      "[111/144] VRSK OK\n",
      "[112/144] VZ OK\n",
      "[113/144] VRTX OK\n",
      "[114/144] VTRS OK\n",
      "[115/144] VICI OK\n",
      "[116/144] V OK\n",
      "[117/144] VST OK\n",
      "[118/144] VMC OK\n",
      "[119/144] WRB OK\n",
      "[120/144] GWW OK\n",
      "[121/144] WAB OK\n",
      "[122/144] WBA OK\n",
      "[123/144] WMT OK\n",
      "[124/144] DIS OK\n",
      "[125/144] WBD OK\n",
      "[126/144] WM OK\n",
      "[127/144] WAT OK\n",
      "[128/144] WEC OK\n",
      "[129/144] WFC OK\n",
      "[130/144] WELL OK\n",
      "[131/144] WST OK\n",
      "[132/144] WDC OK\n",
      "[133/144] WY OK\n",
      "[134/144] WSM OK\n",
      "[135/144] WMB OK\n",
      "[136/144] WTW OK\n",
      "[137/144] WDAY OK\n",
      "[138/144] WYNN OK\n",
      "[139/144] XEL OK\n",
      "[140/144] XYL OK\n",
      "[141/144] YUM OK\n",
      "[142/144] ZBRA OK\n",
      "[143/144] ZBH OK\n",
      "[144/144] ZTS OK\n",
      "Renewed! CSV now up to 2025-10-29\n",
      "Done! Run anytime to renew.\n"
     ]
    }
   ],
   "source": [
    "# --------------------------------------------------------------\n",
    "#  sp500_renew_data.py â€“ Daily Update Script\n",
    "# --------------------------------------------------------------\n",
    "import pandas as pd\n",
    "import yfinance as yf\n",
    "import pickle, os, time\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "# ---------- SETTINGS ----------\n",
    "CSV_FILE    = 'sp500_historical_data.csv'\n",
    "SYMBOL_FILE = 'sp500_symbols.txt'\n",
    "CHECKPOINT  = 'sp500_renew_progress.pkl'\n",
    "END         = datetime.now()\n",
    "START       = END - timedelta(days=731)  # Last two years days (covers weekends)\n",
    "FAILED_TXT  = 'renew_failed.txt'\n",
    "\n",
    "# ---------- 1. Load symbols ----------\n",
    "with open(SYMBOL_FILE) as f:\n",
    "    ALL_SYMBOLS = [s.strip() for s in f if s.strip()]\n",
    "\n",
    "# ---------- 2. Load existing CSV ----------\n",
    "existing_df = pd.read_csv(CSV_FILE, header=[0,1], index_col=0, parse_dates=True)\n",
    "saved_symbols = set(existing_df.columns.levels[0])\n",
    "print(f\"Existing CSV: {len(saved_symbols)} symbols\")\n",
    "\n",
    "# ---------- 3. Resume checkpoint ----------\n",
    "if os.path.exists(CHECKPOINT):\n",
    "    with open(CHECKPOINT, 'rb') as f:\n",
    "        prog = pickle.load(f)\n",
    "    tried = set(prog['tried'])\n",
    "    failed = prog['failed']\n",
    "else:\n",
    "    tried, failed = set(), []\n",
    "\n",
    "to_fetch = [s for s in ALL_SYMBOLS if s not in tried]\n",
    "print(f\"Renewing {len(to_fetch)} symbols...\")\n",
    "\n",
    "# ---------- 4. Fetch function ----------\n",
    "def fetch(sym):\n",
    "    for attempt in range(3):\n",
    "        try:\n",
    "            df = yf.Ticker(sym).history(start=START, end=END)\n",
    "            return df[['Open','High','Low','Close','Volume']] if not df.empty else None\n",
    "        except:\n",
    "            time.sleep(2)\n",
    "    return None\n",
    "\n",
    "# ---------- 5. Loop ----------\n",
    "new_data = {}\n",
    "for i, sym in enumerate(to_fetch, 1):\n",
    "    print(f\"[{i}/{len(to_fetch)}] {sym}\", end=' ')\n",
    "    df = fetch(sym)\n",
    "    if df is not None:\n",
    "        new_data[sym] = df\n",
    "        print(\"OK\")\n",
    "    else:\n",
    "        failed.append(sym)\n",
    "        print(\"FAILED\")\n",
    "    \n",
    "    tried.add(sym)\n",
    "    with open(CHECKPOINT, 'wb') as f:  # Save after EVERY symbol\n",
    "        pickle.dump({'tried': list(tried), 'failed': failed}, f)\n",
    "    time.sleep(1)  # Gentle delay\n",
    "\n",
    "# ---------- 6. Merge & Save ----------\n",
    "if new_data:\n",
    "    new_df = pd.concat(new_data, axis=1, keys=new_data.keys())\n",
    "    # Align dates and merge (only new rows)\n",
    "    combined = pd.concat([existing_df, new_df]).groupby(level=0).last()\n",
    "    combined.to_csv(CSV_FILE)\n",
    "    print(f\"Renewed! CSV now up to {END.date()}\")\n",
    "\n",
    "if failed:\n",
    "    with open(FAILED_TXT, 'w') as f:\n",
    "        f.write('\\n'.join(failed))\n",
    "\n",
    "os.remove(CHECKPOINT)  # Clean up when done\n",
    "print(\"Done! Run anytime to renew.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2ef3a1c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "8b18a699",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading and melting CSV...\n",
      "Ready: 1,255,005 rows\n",
      "Table `sp500_historical_data` created.\n",
      "Loading data...\n",
      "Done! Loaded 1,255,005 rows.\n"
     ]
    }
   ],
   "source": [
    "# sp500_upload_simple.py\n",
    "import getpass\n",
    "import pandas as pd\n",
    "from sqlalchemy import create_engine, text\n",
    "from sqlalchemy.exc import OperationalError\n",
    "\n",
    "# ====================== CONFIG ======================\n",
    "CSV_PATH   = \"sp500_historical_data.csv\"\n",
    "DB_USER    = \"postgres\"\n",
    "DB_PASSWORD = getpass.getpass(\"Password: \")\n",
    "DB_HOST    = \"localhost\"\n",
    "DB_PORT    = \"5432\"\n",
    "DB_NAME    = \"bootcamp_2508_final_project\"\n",
    "TABLE_NAME = \"sp500_historical_data\"\n",
    "\n",
    "# ====================== 1. READ & MELT ======================\n",
    "print(\"Loading and melting CSV...\")\n",
    "df_wide = pd.read_csv(CSV_PATH, header=[0, 1], index_col=0, parse_dates=True, low_memory=False)\n",
    "df_wide.columns = ['_'.join(col).strip() for col in df_wide.columns.values]\n",
    "df_wide = df_wide.reset_index()\n",
    "\n",
    "df_long = pd.melt(\n",
    "    df_wide,\n",
    "    id_vars='Date',\n",
    "    value_vars=[c for c in df_wide.columns if c != 'Date'],\n",
    "    var_name='symbol_metric',\n",
    "    value_name='value'\n",
    ")\n",
    "\n",
    "df_long[['symbol', 'metric']] = df_long['symbol_metric'].str.split('_', expand=True)\n",
    "df_long = df_long.drop(columns='symbol_metric').rename(columns={'Date': 'date'})\n",
    "\n",
    "metric_map = {'Open': 'open', 'High': 'high', 'Low': 'low', 'Close': 'close', 'Volume': 'volume'}\n",
    "df_long['metric'] = df_long['metric'].map(metric_map)\n",
    "\n",
    "df_long = df_long[['symbol', 'date', 'metric', 'value']]\n",
    "df_long = df_long.sort_values(['symbol', 'date', 'metric'])\n",
    "\n",
    "print(f\"Ready: {len(df_long):,} rows\")\n",
    "\n",
    "# ====================== 2. CONNECT ======================\n",
    "engine = create_engine(f\"postgresql://{DB_USER}:{DB_PASSWORD}@{DB_HOST}:{DB_PORT}/{DB_NAME}\")\n",
    "\n",
    "# Ensure DB exists\n",
    "try:\n",
    "    with engine.connect(): pass\n",
    "except OperationalError:\n",
    "    tmp = create_engine(f\"postgresql://{DB_USER}:{DB_PASSWORD}@{DB_HOST}:{DB_PORT}/postgres\")\n",
    "    with tmp.connect() as conn:\n",
    "        conn.execute(\"COMMIT\")\n",
    "        conn.execute(text(f\"CREATE DATABASE {DB_NAME}\"))\n",
    "\n",
    "# ====================== 3. CREATE TABLE ======================\n",
    "with engine.begin() as conn:\n",
    "    conn.execute(text(f\"\"\"\n",
    "        DROP TABLE IF EXISTS {TABLE_NAME};\n",
    "        CREATE TABLE {TABLE_NAME} (\n",
    "            symbol  VARCHAR(10)   NOT NULL,\n",
    "            date    DATE         NOT NULL,\n",
    "            metric  VARCHAR(6)    NOT NULL CHECK (metric IN ('open','high','low','close','volume')),\n",
    "            value   DOUBLE PRECISION,\n",
    "            PRIMARY KEY (symbol, date, metric)\n",
    "        );\n",
    "        CREATE INDEX idx_symbol ON {TABLE_NAME}(symbol);\n",
    "        CREATE INDEX idx_date   ON {TABLE_NAME}(date);\n",
    "    \"\"\"))\n",
    "print(f\"Table `{TABLE_NAME}` created.\")\n",
    "\n",
    "# ====================== 4. LOAD (FAST + REPLACE) ======================\n",
    "print(\"Loading data...\")\n",
    "df_long.to_sql(\n",
    "    TABLE_NAME,\n",
    "    engine,\n",
    "    if_exists='append',   # or 'replace' if you want to wipe first\n",
    "    index=False,\n",
    "    method='multi',\n",
    "    chunksize=10_000\n",
    ")\n",
    "print(f\"Done! Loaded {len(df_long):,} rows.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
