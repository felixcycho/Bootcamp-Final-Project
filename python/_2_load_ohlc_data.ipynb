{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b1ca7370",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 503 stock symbols.\n"
     ]
    }
   ],
   "source": [
    "# Read stock symbols from the file\n",
    "with open('sp500_symbols.txt', 'r') as file:\n",
    "    stock_symbols = [line.strip() for line in file.readlines()]\n",
    "print(f\"Loaded {len(stock_symbols)} stock symbols.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e83a0438",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "__new__() got an unexpected keyword argument 'years'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mTypeError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[14]\u001b[39m\u001b[32m, line 14\u001b[39m\n\u001b[32m     12\u001b[39m CHECKPOINT  = \u001b[33m'\u001b[39m\u001b[33msp500_renew_progress.pkl\u001b[39m\u001b[33m'\u001b[39m\n\u001b[32m     13\u001b[39m END         = datetime.now()\n\u001b[32m---> \u001b[39m\u001b[32m14\u001b[39m START       = END - \u001b[43mtimedelta\u001b[49m\u001b[43m(\u001b[49m\u001b[43myears\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m2\u001b[39;49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Last two years days (covers weekends)\u001b[39;00m\n\u001b[32m     15\u001b[39m FAILED_TXT  = \u001b[33m'\u001b[39m\u001b[33mrenew_failed.txt\u001b[39m\u001b[33m'\u001b[39m\n\u001b[32m     17\u001b[39m \u001b[38;5;66;03m# ---------- 1. Load symbols ----------\u001b[39;00m\n",
      "\u001b[31mTypeError\u001b[39m: __new__() got an unexpected keyword argument 'years'"
     ]
    }
   ],
   "source": [
    "# --------------------------------------------------------------\n",
    "#  sp500_renew_data.py â€“ Daily Update Script\n",
    "# --------------------------------------------------------------\n",
    "import pandas as pd\n",
    "import yfinance as yf\n",
    "import pickle, os, time\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "# ---------- SETTINGS ----------\n",
    "CSV_FILE    = 'sp500_historical_data.csv'\n",
    "SYMBOL_FILE = 'sp500_symbols.txt'\n",
    "CHECKPOINT  = 'sp500_renew_progress.pkl'\n",
    "END         = datetime.now()\n",
    "START       = END - timedelta(year=2)  # Last two years days (covers weekends)\n",
    "FAILED_TXT  = 'renew_failed.txt'\n",
    "\n",
    "# ---------- 1. Load symbols ----------\n",
    "with open(SYMBOL_FILE) as f:\n",
    "    ALL_SYMBOLS = [s.strip() for s in f if s.strip()]\n",
    "\n",
    "# ---------- 2. Load existing CSV ----------\n",
    "existing_df = pd.read_csv(CSV_FILE, header=[0,1], index_col=0, parse_dates=True)\n",
    "saved_symbols = set(existing_df.columns.levels[0])\n",
    "print(f\"Existing CSV: {len(saved_symbols)} symbols\")\n",
    "\n",
    "# ---------- 3. Resume checkpoint ----------\n",
    "if os.path.exists(CHECKPOINT):\n",
    "    with open(CHECKPOINT, 'rb') as f:\n",
    "        prog = pickle.load(f)\n",
    "    tried = set(prog['tried'])\n",
    "    failed = prog['failed']\n",
    "else:\n",
    "    tried, failed = set(), []\n",
    "\n",
    "to_fetch = [s for s in ALL_SYMBOLS if s not in tried]\n",
    "print(f\"Renewing {len(to_fetch)} symbols...\")\n",
    "\n",
    "# ---------- 4. Fetch function ----------\n",
    "def fetch(sym):\n",
    "    for attempt in range(3):\n",
    "        try:\n",
    "            df = yf.Ticker(sym).history(start=START, end=END)\n",
    "            return df[['Open','High','Low','Close','Volume']] if not df.empty else None\n",
    "        except:\n",
    "            time.sleep(2)\n",
    "    return None\n",
    "\n",
    "# ---------- 5. Loop ----------\n",
    "new_data = {}\n",
    "for i, sym in enumerate(to_fetch, 1):\n",
    "    print(f\"[{i}/{len(to_fetch)}] {sym}\", end=' ')\n",
    "    df = fetch(sym)\n",
    "    if df is not None:\n",
    "        new_data[sym] = df\n",
    "        print(\"OK\")\n",
    "    else:\n",
    "        failed.append(sym)\n",
    "        print(\"FAILED\")\n",
    "    \n",
    "    tried.add(sym)\n",
    "    with open(CHECKPOINT, 'wb') as f:  # Save after EVERY symbol\n",
    "        pickle.dump({'tried': list(tried), 'failed': failed}, f)\n",
    "    time.sleep(1)  # Gentle delay\n",
    "\n",
    "# ---------- 6. Merge & Save ----------\n",
    "if new_data:\n",
    "    new_df = pd.concat(new_data, axis=1, keys=new_data.keys())\n",
    "    # Align dates and merge (only new rows)\n",
    "    combined = pd.concat([existing_df, new_df]).groupby(level=0).last()\n",
    "    combined.to_csv(CSV_FILE)\n",
    "    print(f\"Renewed! CSV now up to {END.date()}\")\n",
    "\n",
    "if failed:\n",
    "    with open(FAILED_TXT, 'w') as f:\n",
    "        f.write('\\n'.join(failed))\n",
    "\n",
    "os.remove(CHECKPOINT)  # Clean up when done\n",
    "print(\"Done! Run anytime to renew.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8b18a699",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading and melting CSV...\n",
      "Ready: 1,255,005 rows\n",
      "Table `sp500_historical_data` created.\n",
      "Loading data...\n",
      "Done! Loaded 1,255,005 rows.\n"
     ]
    }
   ],
   "source": [
    "# sp500_upload_simple.py\n",
    "import getpass\n",
    "import pandas as pd\n",
    "from sqlalchemy import create_engine, text\n",
    "from sqlalchemy.exc import OperationalError\n",
    "\n",
    "# ====================== CONFIG ======================\n",
    "CSV_PATH   = \"sp500_historical_data.csv\"\n",
    "DB_USER    = \"postgres\"\n",
    "DB_PASSWORD = getpass.getpass(\"Password: \")\n",
    "DB_HOST    = \"localhost\"\n",
    "DB_PORT    = \"5432\"\n",
    "DB_NAME    = \"bootcamp_2508_final_project\"\n",
    "TABLE_NAME = \"sp500_historical_data\"\n",
    "\n",
    "# ====================== 1. READ & MELT ======================\n",
    "print(\"Loading and melting CSV...\")\n",
    "df_wide = pd.read_csv(CSV_PATH, header=[0, 1], index_col=0, parse_dates=True, low_memory=False)\n",
    "df_wide.columns = ['_'.join(col).strip() for col in df_wide.columns.values]\n",
    "df_wide = df_wide.reset_index()\n",
    "\n",
    "df_long = pd.melt(\n",
    "    df_wide,\n",
    "    id_vars='Date',\n",
    "    value_vars=[c for c in df_wide.columns if c != 'Date'],\n",
    "    var_name='symbol_metric',\n",
    "    value_name='value'\n",
    ")\n",
    "\n",
    "df_long[['symbol', 'metric']] = df_long['symbol_metric'].str.split('_', expand=True)\n",
    "df_long = df_long.drop(columns='symbol_metric').rename(columns={'Date': 'date'})\n",
    "\n",
    "metric_map = {'Open': 'open', 'High': 'high', 'Low': 'low', 'Close': 'close', 'Volume': 'volume'}\n",
    "df_long['metric'] = df_long['metric'].map(metric_map)\n",
    "\n",
    "df_long = df_long[['symbol', 'date', 'metric', 'value']]\n",
    "df_long = df_long.sort_values(['symbol', 'date', 'metric'])\n",
    "\n",
    "print(f\"Ready: {len(df_long):,} rows\")\n",
    "\n",
    "# ====================== 2. CONNECT ======================\n",
    "engine = create_engine(f\"postgresql://{DB_USER}:{DB_PASSWORD}@{DB_HOST}:{DB_PORT}/{DB_NAME}\")\n",
    "\n",
    "# Ensure DB exists\n",
    "try:\n",
    "    with engine.connect(): pass\n",
    "except OperationalError:\n",
    "    tmp = create_engine(f\"postgresql://{DB_USER}:{DB_PASSWORD}@{DB_HOST}:{DB_PORT}/postgres\")\n",
    "    with tmp.connect() as conn:\n",
    "        conn.execute(\"COMMIT\")\n",
    "        conn.execute(text(f\"CREATE DATABASE {DB_NAME}\"))\n",
    "\n",
    "# ====================== 3. CREATE TABLE ======================\n",
    "with engine.begin() as conn:\n",
    "    conn.execute(text(f\"\"\"\n",
    "        DROP TABLE IF EXISTS {TABLE_NAME};\n",
    "        CREATE TABLE {TABLE_NAME} (\n",
    "            symbol  VARCHAR(10)   NOT NULL,\n",
    "            date    DATE         NOT NULL,\n",
    "            metric  VARCHAR(6)    NOT NULL CHECK (metric IN ('open','high','low','close','volume')),\n",
    "            value   DOUBLE PRECISION,\n",
    "            PRIMARY KEY (symbol, date, metric)\n",
    "        );\n",
    "        CREATE INDEX idx_symbol ON {TABLE_NAME}(symbol);\n",
    "        CREATE INDEX idx_date   ON {TABLE_NAME}(date);\n",
    "    \"\"\"))\n",
    "print(f\"Table `{TABLE_NAME}` created.\")\n",
    "\n",
    "# ====================== 4. LOAD (FAST + REPLACE) ======================\n",
    "print(\"Loading data...\")\n",
    "df_long.to_sql(\n",
    "    TABLE_NAME,\n",
    "    engine,\n",
    "    if_exists='append',   # or 'replace' if you want to wipe first\n",
    "    index=False,\n",
    "    method='multi',\n",
    "    chunksize=10_000\n",
    ")\n",
    "print(f\"Done! Loaded {len(df_long):,} rows.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
